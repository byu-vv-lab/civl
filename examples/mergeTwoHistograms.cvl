#include <civlc.h>

/* This verification programmer is based on the merge step implemented
   in the 64-bin implementation of the histogram on the GPU .  I am
   not verifying any host code, I am only focusing on malloc-free GPU
   code.  Trying to check for bounds checking and freedom of
   data-races.
*/

int HIST_BINS = 64;
int NUM_SUB_HIST = 4;

int N = HIST_BINS; /* number of threads */


int NUM_BLOCKS = 1; // this one isn't very important but the least
                    // number of blocks that can be used is 2 not
                    // because the original program specifies that,
                    // but because it is good to make sure that not a
                    // block overwrites the other blocks' output,
                    // inter-block data race checking that is.

int NUM_THREADS = HIST_BINS; // in the original code they were 256
                             // threads, however in the original code
                             // also it was possible to use only 1D
                             // array and index over its elements
                             // using BlockIdx.x and
                             // threadsIdx.x. Here, on the other hand,
                             // I had to make sub-histograms into 2D
                             // arrays since there isn't threadIdx.x
                             // and/or blockIdx.x available to
                             // implement the exact algorithm. This is
                             // certainly a trade-off that *may miss*
                             // some bounds checking against original
                             // code but it can't be done otherwise in
                             // CIVL... 

// outputs
int global_final_merged_hist[HIST_BINS];
int seq_final_merged_hist[HIST_BINS];

// NOTE: I am leaving out the extra tedious details of reading the
// histogram from an image because that is a portion of the code where
// warps, bit-wise operations, bit-masks are used. That portion isn't
// a standard CUDA code and it can't be verified using CIVL as far as
// I know. This is due to warp count, warps sizes, and which specific
// threads fall into which warp based on execution path isn't
// determinable till run time and this is way out of the scope of this
// project that focuses on statically provable properties.




//===================
//  Utility functions
//===================
void verify_func_equivilance(int * hist1, int * hist2)
  $requires HIST_BINS > 0 && NUM_SUB_HIST > 1;
{
  for(int bin = 0; bin < HIST_BINS; bin++)
    {
      $assert hist1[bin] == hist2[bin];
    }
}


//------------------------------------------------------------
// Proved to be difficult now, addresses need more comparisons
// support, please check the README.txt to know more about
// the support needed here. (hint: address compare @<=, @==, 
// ...etc)
// More over, by verifying bounds (i.e. each thread is NOT
// accessing other threads' addresses, there will be no data
// races for this specific algorithm, other algorithms may NOT
// have the same situations since accesses may be not 
// pre-determined as it is the case here, dynamically changing
// they could be).
//------------------------------------------------------------
// data races probably should be done from within each process
//------------------------------------------------------------

// void verify_no_data_races($proc * procs, 
//                        int num_procs,
//                        int * target_modifiable, 
//                        int num_modif_target_elems)
//   $requires num_procs > 0;
// {
//   for(int a_proc = 0; a_proc < HIST_BINS; a_proc++)
//     {
//       for(int another_proc = 0; another_proc < HIST_BINS && a_proc != another_proc; another_proc++) 
//      {
//        $assert a_proc@target_modifiable != another_proc@target_modifiable;
//      }
//     }
// }



//------------------------------------------------------------
// better declare this inside the thread scope since I will be
// checking for threads' bounds only
//------------------------------------------------------------
// void verify_no_out_of_bounds_access($proc procs,
//                                  int stride_no,
//                                  int num_elems_per_proc
//                                  )
// {
//   // get the limits from the global variables  
//    $assert $true;  
// }



//===================
//  initializations                 
//===================
void init(int ** sub_histograms, int value)
{
  // initialize sub histograms
  for(int sub_hist = 0; sub_hist < NUM_SUB_HIST; sub_hist++)
    { // each sub_hist will have a count of 1 in their bins from 0 - 63
      for(int bin = 0; bin< HIST_BINS ; bin++)
        {
          sub_histograms[sub_hist][bin] = value;
        }
    }
}

void init1(int * histogram, int value)
{
  for(int i=0; i < HIST_BINS; i++)
    {
      histogram[i]= value;
    }
}

//==================
// sequential
//==================
/*
  This function sequentially calculates the reference histogram to
  check if it is functionally equivalent with the parallel function.
*/
void seq_histogram_merge(int ** sub_histograms, int * dst_histogram)
{
  for(int bin = 0; bin< HIST_BINS ; bin++)  
    { // 1 bin from all sub-histograms will get added by each thread
      for(int sub_hist = 0; sub_hist < NUM_SUB_HIST; sub_hist++)
        {
          // accumulate sums into final destination
          dst_histogram[bin] = dst_histogram[bin] + sub_histograms[sub_hist][bin];
        }
    }
}
//===================
//  GPU Hist Merge
//===================
void block_compute(int block_id)
{

  int shared_histograms[NUM_SUB_HIST][HIST_BINS];
  int shared_dst_histogram[HIST_BINS];
  int shared_seq_dst_histogram[HIST_BINS];


  int sum = 0; // first version of my verification I missed that there
               // was a reduction sum done by each block (this should
               // introduce even more bugs when I add reduction sum).
  int sum_seq = 0;
  //int sync[NUM_THREADS];
  $proc threads[NUM_THREADS]; // the threads
  int lock = 0; // 0=available
  int in_barrier[N]; // am I inside the barrier?
  int num_in_barrier = 0; // how many are inside barrier?
  int counter = 0; // shared variable used to test barrier

  // generating sub-histograms as if we really filtered them from a
  // data set, after this init no modification to histograms since
  // they are read only.
  init(shared_histograms,1);
  init1(shared_dst_histogram,0);
  init1(shared_seq_dst_histogram,0);

  // sad no debugging available too.
  //  $write "I AM HERE";

  //---------------------------------------------------------------
  // thread compute
  void thread_compute(int tid,int bin)
  {
    // used by __syncthreads() only
    // int am_I_last()
    // {
    //   int sum = 0;
    //   for(int z=0; z < NUM_THREADS; z++)
    //  {
    //    sum += sync[z];
    //  }

    //   if(sum == NUM_THREADS)
    //  return 1; // yes all others here and me, I should let them loose
    //   else
    //  return 0;
    // } // end all_arrived

    //========================
    //  sync+barrier functions
    //========================

    // barrier (like in cuda __syncthreads()
    void __syncthreads(int tid)
    {

      //=======================
      // PRof. Steve's Barrier
      //=======================
      $when (lock==0) lock = 1; // obtains the lock
      in_barrier[tid] = 1; // I am in the barrier
      num_in_barrier++; // increment number in barrier
      if (num_in_barrier == N) { // I am last to enter
        for (int i=0; i<N; i++) in_barrier[i] = 0; // release all
        num_in_barrier = 0; // now none are in barrier
      }
      lock = 0; // release the lock
      $when (in_barrier[tid] == 0); // wait till I am released


      //==============
      // NEW VERSION
      //==============
      // int last_pid;

      // sync[tid] = 1;
      // int all_arrived = sync[tid] == 1 && am_I_last();
      // if(all_arrived)
      //        last_pid = tid;
      // // wait for others to arrive
      // $when(all_arrived == 1) 
      //        {
      //          // here only the last arriving process
      //          if(tid == last_pid) // always process LAST is boss :)
      //            {
      //              for(int k=0; k < NUM_THREADS; k++)
      //                {
      //                  sync[k] = 0; // reset all bells
      //                  // set everyone free, including self
      //                  threads[k]@all_arrived = 1;
      //                }
      //            }
      //        } // do nothing just wait 
      

      // ==============
      // OLD VERSION
      // ==============
      // sync++;

      // $when (sync == count)
      //        {
      //          // allow only process ZERO to reset shared semaphore (sync)
      //          if(tid == 0)
      //            sync=0;
      //        } // resetting counter by all processes

      // is done so that next loop
      // iteration doesn't deadlock

    } // end of barrier


    for(int i=0; i < NUM_SUB_HIST; i++)
      {
        // same bin from each sub histogram is added to its final destination
        shared_dst_histogram[bin] = shared_dst_histogram[bin] + shared_histograms[i][bin];
      }


    // now is the reduction step to sum all of them into one "sum"
    // variable in order that ONLY ONE thread stores it into the global
    // final destination histogram.

    for (int stride = NUM_THREADS / 2; stride > 0; stride / 2) //stride>>=1)
      { 
        //=============================
        // bounds checking
        // I deveoped this while I still get ClassCastException so
        // it could be buggy, this is the best I could do without
        // testing ...
        //=============================
        void check_bounds()
        {
          // checks for per thread vs per sub-histogram bounds
          $assert tid + stride >= 0 && tid + stride < HIST_BINS * stride;

          // checks for bounds against stride size, not to add
          // values more than once
          $assert 0 <= tid && tid < stride;

          // checks that I am NOT generating more threads that
          // eventually will access array out-of-bounds addresses
          $assert 0 <= tid && tid < NUM_THREADS;
              
          // like the one before but for sub-histograms accesses
          // in the 2D array
          $assert 0 <= stride && stride < NUM_SUB_HIST;

          // while this is redundant, it is here to first affirm
          // my logic is right, and to indicate it is needed
          // in case NUM_THREADS != HIST_BINS
          $assert 0 <= tid && tid < HIST_BINS;

          // yay! I did it without using address-of operators!
        }
        //=============================

        $assert NUM_THREADS%2 == 0; // so that the reduction works as
                                    // intended
        __syncthreads(tid);

        // this if prevents from double summing an element more than
        // once to the reduction result.
        if(tid < stride)
          {
            check_bounds();
            shared_dst_histogram[tid] += shared_dst_histogram[tid + stride];
          } // end-if
      } // end reduction for-loop
  } // end thread_compute

  //---------------------------------------------------------------

  // launching threads to merge sub histograms generated in parallel
  // to a destination shared per block histogram

  // The reason I limited the threads to be exactly like number of
  // bins are two:
  // 1- I can't find a way in CIVL to use thread ID's (they don't
  // exist) so that I limit the range threads should be operating on
  // reduction step.
  // 2- Because of above reason, I had to make sub histograms 2D array
  // in which each array is a sub-histogram, to ease indexing and
  // reduction step using CIVL. 
  
  threads[0] = $self;
  for(int i= 1; i < HIST_BINS; i++)
    {
      threads[i] = $spawn thread_compute(i);
      // threadIds[i] = i;

    } // done parallel sum to bins in block-wise histogram


  int mutex = 0; // one lock is enough since only one thread should
                 // access the critical area, and others should skip.  
  if(mutex == 0)
    {
      mutex = 1;
      // --------------------------------
      // Functional Equivalence for 
      // intermediate sub-hists merging
      // --------------------------------
      // compute reference sequential histogram
      seq_histogram_merge(shared_histograms, shared_seq_dst_histogram);

      // Each block checks by itself for functional equivalence
      verify_func_equivilance(shared_seq_dst_histogram, shared_dst_histogram);

      // --------------------------------
      // Functional Equivilance for 
      // final reduction step
      // --------------------------------

      // do sequential reduction
      for(int i=0; i < HIST_BINS; i++)
        {
          sum_seq += shared_seq_dst_histogram[i];
        }
      sum = shared_dst_histogram[0]; // the reduced sum is stored at
                                     // index zero of the array over
                                     // which reduction happened
      // functional equivilance for the reduction step
      $assert sum = sum_seq;
      // Now put the result in its final global memory destination
      global_final_merged_hist[block_id] = sum;

    } // don't release the lock, all processes should skip above scope except one

} // end of block compute

//===============
// MAIN
//===============

// MAIN represents a "device-wise" execution grid (or the host side
// interface to this kernel).
void main()
{

  // initialization
  init1(global_final_merged_hist,0);
  
  //========================
  // Parallel Merge
  //========================

  // a device launches couple of thread blocks to operate on its
  // Global memory. Now we are using main() as the device
  for(int block_id = 0; block_id < NUM_BLOCKS; block_id++)
    {
      // each block is executed to exhaustion and then another block
      // is launched. Reason follows
      block_compute(block_id);

      // You can notice here that each block is NOT run in parallel
      // with other blocks, why? this is because each final block
      // output will share all addresses with the same other
      // block. While the real device will execute them in parallel, I
      // chose to execute them in sequence since it would take much
      // more logic to prove that no thread from any block is really
      // racing with other threads from another block on the same
      // final destination global histogram/array of counts.
    }

  //---------------------------
  //Extra notes for my self
  //---------------------------

  // functional equivilance and bounds checking was done on the block
  // level. Each block will essentially do the same thing and if one
  // didn't fail, others won't. Also, since block are run in
  // sequential manner (one to exhaustion and then another), we don't
  // have to check for inter-block data races, in reality hoever it
  // MUST be done but time didint permit to verify the remaining
  // lavels of execution/memory hierarchies verification.

  // Race conditions can arise in two hierarchies: block scope and
  // device scope. So, no need to do anything more in this level.

  // Out of bounds accesses can happen on: threads level, blocks level,
  // even device level. So, I will be checking only the threads
  // level.
  
}

//  LocalWords:  civlc malloc NUM BlockIdx threadsIdx threadIdx CIVL
//  LocalWords:  blockIdx
